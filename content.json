{"meta":{"title":"hi coder","subtitle":"a blog","description":"东南有大树的博客","author":"东南有大树","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"北京线下活动-个人管理与精进","slug":"线下活动","date":"2019-10-12T01:50:23.000Z","updated":"2019-10-12T08:18:18.763Z","comments":true,"path":"2019/10/12/线下活动/","link":"","permalink":"http://yoursite.com/2019/10/12/线下活动/","excerpt":"","text":"活动大纲活动主题：个人管理与精进活动时间：2019年10月13日14时活动地址：昆山秋水参会人员数量：30人组织人员：石起起、杨生、崔鑫赞助方：光环国际学校活动流程： 12:30 ~ 13:30 设备调试与彩排 13:30 ~ 14:00 签到 14:00 ~ 14:20 互相认识，然后划分小组 14:20 ~ 16:40 主题分支讨论 16:40 ~ 17:00 合影留念，散会 活动说明本次活动有6个子主题，计划将30人分为敏捷的5人小组，依次对题目发起讨论。讨论完成后，小组推荐一位代表上台表达对主题的理解与经验分享，并挑选一到两名其它小组成员进行互动。 在活动过程中，将采集大家的精彩议论与有趣图片，在此博文中更新。 6个主题 如何管理时间 如何规划财务 如何处理关系 如何平衡生活 如何高效工作 如何管理健康 目前，博客站点还在搭建中，有许多不完善的地方，望多多指教！ \b","categories":[],"tags":[{"name":"pmp","slug":"pmp","permalink":"http://yoursite.com/tags/pmp/"}]},{"title":"用于分类的线性模型：Logistic回归与线性支持向量机","slug":"线性回归","date":"2019-10-11T12:00:00.000Z","updated":"2019-10-12T14:41:32.425Z","comments":true,"path":"2019/10/11/线性回归/","link":"","permalink":"http://yoursite.com/2019/10/11/线性回归/","excerpt":"","text":"线性模型用于分类，分类的原理还要从公式说起。 $y = w[0]x[0]+w[1]x[1]+…+w[p]*x[p]+b$ 线性模型的公司的结果y是一个连续的输出结果，如果将y的值与0作一个界限上的区分，那y的值将被分成两个区域。公式的表达如下： $y = w[0]x[0]+w[1]x[1]+…+w[p]*x[p]+b&gt;0$ 也就是说，如果该公式（函数）预测的结果值小于0，就归类为-1，如果结果值大于0，就归类为1. 需要重点理解的地方在于： 对于用于回归的线性模型来说，输出y是特征的线性函数，是直线、平面或超平面（对于更高维的数据集）。 对于用于分类的线性模型，决策边界是输入的线性函数。 不同线性模型算法之间的区别在于： 系数和截距的特定组合对训练集数据拟合好坏的度量方法； 是否使用正则化，以及使用哪种正则化方法。 最觉的两种线性分类算法是Logistic回归和线性支持向量机。前者在linear_model.LogisticRegression中实现，后者在svm.LinearSVC（SVC代表支持再是分类器）中实现 。 接下来对这两个模型做一个初步认识吧！ 12345# 在学习之前，先导入这些常用的模块import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport mglearn Logistic 与 LinearSVC 模型下面，将这两个模型用在二分类数据forge数据集上，并显示模型得出的决策边界。 先展示下二分类的展示图： 1234# 导入二分类数据X, y = mglearn.datasets.make_forge()mglearn.discrete_scatter(X[:, 0], X[:, 1], y)plt.show() 12345678910111213141516171819202122232425262728# 导入Logisticfrom sklearn.linear_model import LogisticRegression# 导入LinearSVCfrom sklearn.svm import LinearSVC# 导入二分类数据X, y = mglearn.datasets.make_forge()# 建立一个幕布，两个绘图区fig, axes = plt.subplots(1, 2, figsize=(10, 3))# 分别在两个绘图区上绘制两个模型的决策边界for model, ax in zip([LinearSVC(), LogisticRegression()], axes): # 在模型上训练数据 clf = model.fit(X, y) # 绘制决策边界 mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=.5, ax=ax, alpha=.7) # 绘制二分类的训练数据 mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax) # 设置标题为模型名称 ax.set_title('&#123;&#125;'.format(clf.__class__.__name__)) # 设置x坐标轴标签名称 ax.set_xlabel('Feature 0') # 设置y坐标轴标签名称 ax.set_ylabel('Feature 1')# 在第一个绘图区上显示图例axes[0].legend()plt.show() 说明： forge数据集有两个特征，分别对应X轴与Y轴。 LinearSVC与LogisticRegression得到的决策边界，都是直线，将数据分为了上下两个区域。 两个模型默认使用了L2正则化。 继续探讨： 对于LinearSVC与LogisticRegression而言，决定正则化强度的权衡参数叫做C。C值越大，对就的正则化越弱。 也就是说，越大的C将拥有越上的约束力，即系数的大小更自由，模型对于数据的贴合度将变得更复杂。 如果C越小，则对系数的约束越大，甚至趋向于0，使模型更能贴合大多数数据，模型也更简单。 下面直接展示一下LinearSVC模型的C分别取0.01、1、100时模型的决策边界效果： 1mglearn.plots.plot_linear_svc_regularization() 对上图的总结： 最左侧的图，C值很小，所以对应强正则化。要求模型更贴合于大多数数据，因此对于图中两个错误的点进行了忽略。 中间的图，C值稍大，由于对模型的约束变小，模型对于数据的反应则变得更加第三一些，决策线向着两个错误的点进行偏移，希望更好的贴合训练数据。 右侧图C值很大，因为对模型的约束很小，导致模型的决策边界要求对所有数据都贴合，造成了过拟合现象。 再来看看使用乳腺癌数据集对LogisticRegression模型做出分析： 1234567891011121314# 导入乳腺癌数据from sklearn.datasets import load_breast_cancercancer = load_breast_cancer()# 将数据分为训练集测试集from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)# 使用模型训练数据logreg = LogisticRegression().fit(X_train, y_train)# 查看模型的评估值print('训练集评估分数：', logreg.score(X_train, y_train))print('测试集评估分数：', logreg.score(X_test, y_test)) 训练集评估分数： 0.9553990610328639 测试集评估分数： 0.958041958041958这里模型置信的C值是1.如果训练集的评分与测试集的评分差不多，那可能存在欠拟合的现象。 现在给模型增大C值，再看看评估结果： 123logreg100 = LogisticRegression(C=100).fit(X_train, y_train)print('训练集评估分数：', logreg100.score(X_train, y_train))print('测试集评估分数：', logreg100.score(X_test, y_test)) 训练集评估分数： 0.971830985915493 测试集评估分数： 0.965034965034965通过增大模型的C值，发现模型的精度变高了。 现在再减小C值看看模型的评估分数： 123logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)print('训练集评估分数：', logreg001.score(X_train, y_train))print('测试集评估分数：', logreg001.score(X_test, y_test)) 训练集评估分数： 0.9342723004694836 测试集评估分数： 0.9300699300699301可以看出，模型的精度变小了，并且存在欠拟合的可能。 按照老办法，我们当不同C值情况下，模型得出的系数图例化，看看结果： 1234567891011plt.plot(logreg.coef_.T, 'o', label='C=1')plt.plot(logreg100.coef_.T, '^', label='C=100')plt.plot(logreg001.coef_.T, 'v', label='C=0.01')plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)plt.hlines(0, 0, cancer.data.shape[1])plt.ylim(-5, 5)plt.xlabel('Codefficient index')plt.ylabel('Coefficient magnitude')plt.legend()plt.show() 从这个图例可以看出，C值越小，模型的系数就越趋向于0. 另外，该模型默认使用L2正则，也可以将其改成L1正则，以减少模型使用的特征： 12345678910111213141516# 评估for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']): lr_l1 = LogisticRegression(C=C, penalty='l1').fit(X_train, y_train) print('训练集，C=&#123;0&#125;,评估分数=&#123;1&#125;'.format(C, lr_l1.score(X_train, y_train))) print('测试集，C=&#123;0&#125;,评估分数=&#123;1&#125;'.format(C, lr_l1.score(X_test, y_test))) # 绘图 plt.plot(lr_l1.coef_.T, marker, label='C=&#123;&#125;'.format(C)) plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)plt.hlines(0, 0, cancer.data.shape[1])plt.ylim(-5, 5)plt.xlabel('Codefficient index')plt.ylabel('Coefficient magnitude')plt.legend(loc=3)plt.show() 训练集，C=0.001,评估分数=0.9131455399061033 测试集，C=0.001,评估分数=0.9230769230769231 训练集，C=1,评估分数=0.960093896713615 测试集，C=1,评估分数=0.958041958041958 训练集，C=100,评估分数=0.9859154929577465 测试集，C=100,评估分数=0.9790209790209791 Ok，上图对于不同C值所对应的L正则下的系数分布有了直观的了解，也就明白了对应的约束力。 这里主要需要明白设置L1或L2需要通过参数penalty来设置。 用于多分类的线性模型多分类其实也是一处二分类的模式，它是一对其余的方法。 这里展示一个三类的gf数据： 12345678from sklearn.datasets import make_blobsX, y = make_blobs(random_state=42)mglearn.discrete_scatter(X[:, 0], X[:, 1], y)plt.xlabel('Feature 0')plt.ylabel('Feature 1')plt.legend(['Class 0', 'Class 1', 'Class 2'])plt.show() 然后使用该数据对LinearSVC分类器进行训练： 123linear_svm = LinearSVC().fit(X, y)print('模型斜率集：', linear_svm.coef_.shape)print('模型截距集：', linear_svm.intercept_.shape) 模型斜率集： (3, 2) 模型截距集： (3,)通过形状可以明白：斜率集有3行，每行代表类别之一的一个系数向量；有2列，每列包含某个特征对应的系数值。而截距是个一维数据，包含每个类别的截距值。 现在将分类器给出的直线进行可视化： 12345678910mglearn.discrete_scatter(X[:, 0], X[:, 1], y)line = np.linspace(-15, 15)for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_, ['b', 'r', 'g']): plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)plt.ylim(-10, 15)plt.xlim(-10, 8)plt.xlabel('Feature 0')plt.ylabel('Feature 1')plt.legend(['Class 0', 'Class 1', 'Class 2'], loc=(1.01, 0.3))plt.show() 在这里，线条的颜色与各点的颜色是一致的。从图中可以很直观的看到这三个点是如何被分成三类的。 但是，这三条线交叉的地方，有一个空白的三角区，那这个区域属于哪个类别呢？ 答案是分类方程结果最大的那个类别，即最接近的那条结对应的类别！ 下面将展示整个二维空间是如何被分类的： 123456789mglearn.plots.plot_2d_classification(linear_svm, X, alpha=.7)mglearn.discrete_scatter(X[:, 0], X[:, 1], y)line = np.linspace(-15, 15)for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_, ['b', 'r', 'g']): plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)plt.xlabel('Feature 0')plt.ylabel('Feature 1')plt.legend(['Class 0', 'Class 1', 'Class 2'], loc=(1.01, 0.3))plt.show() 通过上图很明白的就看出中间的三角区是如何被分类的了！","categories":[],"tags":[{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"}]},{"title":"量化投资入门指南-视频教程","slug":"量化投资入门指南","date":"2019-10-10T12:00:00.000Z","updated":"2019-10-12T07:31:30.390Z","comments":true,"path":"2019/10/10/量化投资入门指南/","link":"","permalink":"http://yoursite.com/2019/10/10/量化投资入门指南/","excerpt":"","text":"课程链接-网易云课堂 适用人群量化交易入门者。 课程概述没有Python基础，没有量化概念，都不是问题，本课程是专门为编程零基础、量化交易零基础者而打造。利用当下最火热的编程语言Python，对证券标的的价格信息进行分析处理，一步一步完成策略的编写！ \b","categories":[],"tags":[{"name":"quant","slug":"quant","permalink":"http://yoursite.com/tags/quant/"}]},{"title":"关于我","slug":"关于我","date":"2019-10-10T02:50:23.000Z","updated":"2019-10-12T07:27:04.878Z","comments":true,"path":"2019/10/10/关于我/","link":"","permalink":"http://yoursite.com/2019/10/10/关于我/","excerpt":"","text":"暂无。","categories":[],"tags":[{"name":"life","slug":"life","permalink":"http://yoursite.com/tags/life/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-10-10T01:50:23.000Z","updated":"2019-10-12T07:18:18.968Z","comments":true,"path":"2019/10/10/hello-world/","link":"","permalink":"http://yoursite.com/2019/10/10/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]}]}