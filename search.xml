<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>北京线下活动-个人管理与精进</title>
      <link href="/2019/10/12/%E7%BA%BF%E4%B8%8B%E6%B4%BB%E5%8A%A8/"/>
      <url>/2019/10/12/%E7%BA%BF%E4%B8%8B%E6%B4%BB%E5%8A%A8/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/malism.png" alt></p><a id="more"></a><h2 id="活动大纲"><a href="#活动大纲" class="headerlink" title="活动大纲"></a>活动大纲</h2><p>活动主题：个人管理与精进<br>活动时间：2019年10月13日14时<br>活动地址：昆山秋水<br>参会人员数量：30人<br>组织人员：石起起、杨生、崔鑫<br>赞助方：光环国际学校<br>活动流程：</p><ul><li>12:30 ~ 13:30 设备调试与彩排</li><li>13:30 ~ 14:00 签到</li><li>14:00 ~ 14:20 互相认识，然后划分小组</li><li>14:20 ~ 16:40 主题分支讨论</li><li>16:40 ~ 17:00 合影留念，散会</li></ul><h2 id="活动说明"><a href="#活动说明" class="headerlink" title="活动说明"></a>活动说明</h2><p>本次活动有6个子主题，计划将30人分为敏捷的5人小组，依次对题目发起讨论。讨论完成后，小组推荐一位代表上台表达对主题的理解与经验分享，并挑选一到两名其它小组成员进行互动。</p><p>在活动过程中，将采集大家的精彩议论与有趣图片，在此博文中更新。</p><h2 id="6个主题"><a href="#6个主题" class="headerlink" title="6个主题"></a>6个主题</h2><ol><li>如何管理时间</li><li>如何规划财务</li><li>如何处理关系</li><li>如何平衡生活</li><li>如何高效工作</li><li>如何管理健康</li></ol><h2 id="活动PPT展示"><a href="#活动PPT展示" class="headerlink" title="活动PPT展示"></a>活动PPT展示</h2><p><a href="https://shimo.im/slides/KtTXc9cvCWrDW9Tv/" target="_blank" rel="noopener">https://shimo.im/slides/KtTXc9cvCWrDW9Tv/</a></p><h2 id="精彩活动记录"><a href="#精彩活动记录" class="headerlink" title="精彩活动记录"></a>精彩活动记录</h2><p>刚开始的时候，大家各坐在自己的座位上，都在默默的刷手机。直到音乐停下来。在杨生的帮助下，大家错落分开，划分为五个小组。真是感谢这些冒着雨水跑来的小伙伴们。（其实都是大佬~）</p><p>通过抽签，各小组拿到了属于自己小组的讨论主题。好像火柴被猛烈的划过，大家的热情一下子燃烧起来。讨论声、笑声此起彼伏。</p><p><img src="/images/%E5%85%89%E7%8E%AF/IMG_9316.JPG" alt></p><p>第一位上场的大哥很霸气，分享的知识是关于时间管理方面的。从他的身上，让人感受到了运筹帷幄的控制感。他说话很幽默，而且笑容真的很亲善。</p><p><img src="/images/%E5%85%89%E7%8E%AF/IMG_3916.JPG" alt></p><p>由于时间有限，并且只划分了五个小组，抽签的时候，第二个主题没有被抽走。所以直接跳到第三个话题。</p><p>这是个十分谦虚的美女。活动过程中，她向大家请教过很多问题，可当她上台后，我们被她的气场完全hold住了。用一个词来形容她就是——学霸。她分享的主题内容简直可以载入教科书。</p><p><img src="/images/%E5%85%89%E7%8E%AF/IMG_6941.JPG" alt></p><p>分享第四个主题的美女真的是帮了我们的大忙了。她是最早来的女生，然后开始帮助我们整理会场，负责签到。（感谢啦~）</p><p>从她的分享内容与会场表现来看，她那小小的身躯里有着无比强大的自律精神。非常值得我们学习。</p><p><img src="/images/%E5%85%89%E7%8E%AF/IMG_3405.JPG" alt></p><p>分享第五个主题的小伙为了我们提出了两个个人事务管理、高效工作的方法与工具：GTD和番茄工作法。会后也积极与我联系，表达了他对这两种工作方式的浓厚兴趣。如果有对此方法感兴趣的学友，可以找这位帅哥聊聊哦！</p><p><img src="/images/%E5%85%89%E7%8E%AF/IMG_7742.JPG" alt></p><p>最后上场的这们美女有点怯场，但她还是勇敢的走到了台前。很棒，你是最值得我们点赞的一位，加油！</p><p><img src="/images/%E5%85%89%E7%8E%AF/IMG_5983.JPG" alt></p><p>17点的时候，我们准时结束了活动，大家一起合影，然后各奔东西。这个世界，就是这样，有时候很大，有时候很小。愿各位生活一切顺利。</p><blockquote><p>ps: 最后还要感谢崔鑫同学，她是光环的工作人员，背后一直是她在默默替这场活动做协调。敬最可爱的人！</p></blockquote><p></p>]]></content>
      
      
      
        <tags>
            
            <tag> pmp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用于分类的线性模型：Logistic回归与线性支持向量机</title>
      <link href="/2019/10/11/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
      <url>/2019/10/11/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<p><img src="https://blog.trello.com/hs-fs/hubfs/Flow-state-final.png?width=2400&name=Flow-state-final.png" alt></p><p>线性模型用于分类，分类的原理还要从公式说起。</p><p>$y = w[0]<em>x[0]+w[1]</em>x[1]+…+w[p]*x[p]+b$</p><p>线性模型的公司的结果<code>y</code>是一个连续的输出结果，如果将<code>y</code>的值与<code>0</code>作一个界限上的区分，那<code>y</code>的值将被分成两个区域。公式的表达如下：</p><p>$y = w[0]<em>x[0]+w[1]</em>x[1]+…+w[p]*x[p]+b&gt;0$</p><p>也就是说，如果该公式（函数）预测的结果值小于0，就归类为-1，如果结果值大于0，就归类为1.</p><p><strong>需要重点理解的地方在于：</strong></p><ul><li>对于用于回归的线性模型来说，输出<code>y</code>是特征的线性函数，是直线、平面或超平面（对于更高维的数据集）。</li><li>对于用于分类的线性模型，<strong>决策边界</strong>是输入的线性函数。</li></ul><p><strong>不同线性模型算法之间的区别在于：</strong></p><ul><li>系数和截距的特定组合对训练集数据拟合好坏的度量方法；</li><li>是否使用正则化，以及使用哪种正则化方法。</li></ul><p>最觉的两种线性分类算法是<strong>Logistic回归</strong>和<strong>线性支持向量机</strong>。前者在<code>linear_model.LogisticRegression</code>中实现，后者在<code>svm.LinearSVC</code>（SVC代表支持再是分类器）中实现 。</p><p>接下来对这两个模型做一个初步认识吧！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在学习之前，先导入这些常用的模块</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br></pre></td></tr></table></figure><h2 id="Logistic-与-LinearSVC-模型"><a href="#Logistic-与-LinearSVC-模型" class="headerlink" title="Logistic 与 LinearSVC  模型"></a>Logistic 与 LinearSVC  模型</h2><p>下面，将这两个模型用在二分类数据<code>forge</code>数据集上，并显示模型得出的决策边界。</p><p>先展示下二分类的展示图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入二分类数据</span></span><br><span class="line">X, y = mglearn.datasets.make_forge()</span><br><span class="line">mglearn.discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/5787343-5fa81d6958c2e19d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入Logistic</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment"># 导入LinearSVC</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入二分类数据</span></span><br><span class="line">X, y = mglearn.datasets.make_forge()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立一个幕布，两个绘图区</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别在两个绘图区上绘制两个模型的决策边界</span></span><br><span class="line"><span class="keyword">for</span> model, ax <span class="keyword">in</span> zip([LinearSVC(), LogisticRegression()], axes):</span><br><span class="line">    <span class="comment"># 在模型上训练数据</span></span><br><span class="line">    clf = model.fit(X, y)</span><br><span class="line">    <span class="comment"># 绘制决策边界</span></span><br><span class="line">    mglearn.plots.plot_2d_separator(clf, X, fill=<span class="literal">False</span>, eps=<span class="number">.5</span>, ax=ax, alpha=<span class="number">.7</span>)</span><br><span class="line">    <span class="comment"># 绘制二分类的训练数据</span></span><br><span class="line">    mglearn.discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], y, ax=ax)</span><br><span class="line">    <span class="comment"># 设置标题为模型名称</span></span><br><span class="line">    ax.set_title(<span class="string">'&#123;&#125;'</span>.format(clf.__class__.__name__))</span><br><span class="line">    <span class="comment"># 设置x坐标轴标签名称</span></span><br><span class="line">    ax.set_xlabel(<span class="string">'Feature 0'</span>)</span><br><span class="line">    <span class="comment"># 设置y坐标轴标签名称</span></span><br><span class="line">    ax.set_ylabel(<span class="string">'Feature 1'</span>)</span><br><span class="line"><span class="comment"># 在第一个绘图区上显示图例</span></span><br><span class="line">axes[<span class="number">0</span>].legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/5787343-742816b9fee5657e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p><strong>说明：</strong></p><ul><li>forge数据集有两个特征，分别对应X轴与Y轴。</li><li>LinearSVC与LogisticRegression得到的决策边界，都是直线，将数据分为了上下两个区域。</li><li>两个模型默认使用了L2正则化。</li></ul><p><strong>继续探讨：</strong></p><ul><li>对于LinearSVC与LogisticRegression而言，决定正则化强度的权衡参数叫做<code>C</code>。<code>C</code>值越大，对就的正则化<strong>越弱</strong>。</li><li>也就是说，越大的<code>C</code>将拥有越上的约束力，即系数的大小更自由，模型对于数据的贴合度将变得更复杂。</li><li>如果<code>C</code>越小，则对系数的约束越大，甚至趋向于0，使模型更能贴合大多数数据，模型也更简单。</li></ul><p>下面直接展示一下<strong>LinearSVC</strong>模型的<code>C</code>分别取<code>0.01</code>、<code>1</code>、<code>100</code>时模型的决策边界效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mglearn.plots.plot_linear_svc_regularization()</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/5787343-fed562cbcbbcd23e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p><strong>对上图的总结：</strong></p><ol><li>最左侧的图，C值很小，所以对应强正则化。要求模型更贴合于大多数数据，因此对于图中两个错误的点进行了忽略。</li><li>中间的图，C值稍大，由于对模型的约束变小，模型对于数据的反应则变得更加第三一些，决策线向着两个错误的点进行偏移，希望更好的贴合训练数据。</li><li>右侧图C值很大，因为对模型的约束很小，导致模型的决策边界要求对所有数据都贴合，造成了过拟合现象。</li></ol><p><strong>再来看看使用乳腺癌数据集对LogisticRegression模型做出分析：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入乳腺癌数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据分为训练集测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用模型训练数据</span></span><br><span class="line">logreg = LogisticRegression().fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看模型的评估值</span></span><br><span class="line">print(<span class="string">'训练集评估分数：'</span>, logreg.score(X_train, y_train))</span><br><span class="line">print(<span class="string">'测试集评估分数：'</span>, logreg.score(X_test, y_test))</span><br></pre></td></tr></table></figure><pre><code>训练集评估分数： 0.9553990610328639测试集评估分数： 0.958041958041958</code></pre><p>这里模型置信的<code>C</code>值是1.如果训练集的评分与测试集的评分差不多，那可能存在欠拟合的现象。</p><p>现在给模型增大<code>C</code>值，再看看评估结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">logreg100 = LogisticRegression(C=<span class="number">100</span>).fit(X_train, y_train)</span><br><span class="line">print(<span class="string">'训练集评估分数：'</span>, logreg100.score(X_train, y_train))</span><br><span class="line">print(<span class="string">'测试集评估分数：'</span>, logreg100.score(X_test, y_test))</span><br></pre></td></tr></table></figure><pre><code>训练集评估分数： 0.971830985915493测试集评估分数： 0.965034965034965</code></pre><p>通过增大模型的<code>C</code>值，发现模型的精度变高了。</p><p>现在再减小<code>C</code>值看看模型的评估分数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">logreg001 = LogisticRegression(C=<span class="number">0.01</span>).fit(X_train, y_train)</span><br><span class="line">print(<span class="string">'训练集评估分数：'</span>, logreg001.score(X_train, y_train))</span><br><span class="line">print(<span class="string">'测试集评估分数：'</span>, logreg001.score(X_test, y_test))</span><br></pre></td></tr></table></figure><pre><code>训练集评估分数： 0.9342723004694836测试集评估分数： 0.9300699300699301</code></pre><p>可以看出，模型的精度变小了，并且存在欠拟合的可能。</p><p>按照老办法，我们当不同<code>C</code>值情况下，模型得出的系数图例化，看看结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(logreg.coef_.T, <span class="string">'o'</span>, label=<span class="string">'C=1'</span>)</span><br><span class="line">plt.plot(logreg100.coef_.T, <span class="string">'^'</span>, label=<span class="string">'C=100'</span>)</span><br><span class="line">plt.plot(logreg001.coef_.T, <span class="string">'v'</span>, label=<span class="string">'C=0.01'</span>)</span><br><span class="line"></span><br><span class="line">plt.xticks(range(cancer.data.shape[<span class="number">1</span>]), cancer.feature_names, rotation=<span class="number">90</span>)</span><br><span class="line">plt.hlines(<span class="number">0</span>, <span class="number">0</span>, cancer.data.shape[<span class="number">1</span>])</span><br><span class="line">plt.ylim(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Codefficient index'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Coefficient magnitude'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/5787343-d3746920534af4d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>从这个图例可以看出，<code>C</code>值越小，模型的系数就越趋向于0.</p><p>另外，该模型默认使用<strong>L2</strong>正则，也可以将其改成<strong>L1</strong>正则，以减少模型使用的特征：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 评估</span></span><br><span class="line"><span class="keyword">for</span> C, marker <span class="keyword">in</span> zip([<span class="number">0.001</span>, <span class="number">1</span>, <span class="number">100</span>], [<span class="string">'o'</span>, <span class="string">'^'</span>, <span class="string">'v'</span>]):</span><br><span class="line">    lr_l1 = LogisticRegression(C=C, penalty=<span class="string">'l1'</span>).fit(X_train, y_train)</span><br><span class="line">    print(<span class="string">'训练集，C=&#123;0&#125;,评估分数=&#123;1&#125;'</span>.format(C, lr_l1.score(X_train, y_train)))</span><br><span class="line">    print(<span class="string">'测试集，C=&#123;0&#125;,评估分数=&#123;1&#125;'</span>.format(C, lr_l1.score(X_test, y_test)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 绘图</span></span><br><span class="line">    plt.plot(lr_l1.coef_.T, marker, label=<span class="string">'C=&#123;&#125;'</span>.format(C))</span><br><span class="line">    </span><br><span class="line">plt.xticks(range(cancer.data.shape[<span class="number">1</span>]), cancer.feature_names, rotation=<span class="number">90</span>)</span><br><span class="line">plt.hlines(<span class="number">0</span>, <span class="number">0</span>, cancer.data.shape[<span class="number">1</span>])</span><br><span class="line">plt.ylim(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Codefficient index'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Coefficient magnitude'</span>)</span><br><span class="line">plt.legend(loc=<span class="number">3</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>训练集，C=0.001,评估分数=0.9131455399061033测试集，C=0.001,评估分数=0.9230769230769231训练集，C=1,评估分数=0.960093896713615测试集，C=1,评估分数=0.958041958041958训练集，C=100,评估分数=0.9859154929577465测试集，C=100,评估分数=0.9790209790209791</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/5787343-ffd93333677d2ed9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>Ok，上图对于不同<code>C</code>值所对应的<code>L</code>正则下的系数分布有了直观的了解，也就明白了对应的约束力。</p><p>这里主要需要明白设置<strong>L1</strong>或<strong>L2</strong>需要通过参数<code>penalty</code>来设置。</p><h2 id="用于多分类的线性模型"><a href="#用于多分类的线性模型" class="headerlink" title="用于多分类的线性模型"></a>用于多分类的线性模型</h2><p>多分类其实也是一处二分类的模式，它是<strong>一对其余</strong>的方法。</p><p>这里展示一个三类的gf数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line">X, y = make_blobs(random_state=<span class="number">42</span>)</span><br><span class="line">mglearn.discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], y)</span><br><span class="line">plt.xlabel(<span class="string">'Feature 0'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Feature 1'</span>)</span><br><span class="line">plt.legend([<span class="string">'Class 0'</span>, <span class="string">'Class 1'</span>, <span class="string">'Class 2'</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/5787343-ceeb7cd87f9a2655.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>然后使用该数据对<code>LinearSVC</code>分类器进行训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">linear_svm = LinearSVC().fit(X, y)</span><br><span class="line">print(<span class="string">'模型斜率集：'</span>, linear_svm.coef_.shape)</span><br><span class="line">print(<span class="string">'模型截距集：'</span>, linear_svm.intercept_.shape)</span><br></pre></td></tr></table></figure><pre><code>模型斜率集： (3, 2)模型截距集： (3,)</code></pre><p>通过形状可以明白：斜率集有3行，每行代表类别之一的一个系数向量；有2列，每列包含某个特征对应的系数值。而截距是个一维数据，包含每个类别的截距值。</p><p>现在将分类器给出的直线进行可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mglearn.discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], y)</span><br><span class="line">line = np.linspace(<span class="number">-15</span>, <span class="number">15</span>)</span><br><span class="line"><span class="keyword">for</span> coef, intercept, color <span class="keyword">in</span> zip(linear_svm.coef_, linear_svm.intercept_, [<span class="string">'b'</span>, <span class="string">'r'</span>, <span class="string">'g'</span>]):</span><br><span class="line">    plt.plot(line, -(line * coef[<span class="number">0</span>] + intercept) / coef[<span class="number">1</span>], c=color)</span><br><span class="line">plt.ylim(<span class="number">-10</span>, <span class="number">15</span>)</span><br><span class="line">plt.xlim(<span class="number">-10</span>, <span class="number">8</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Feature 0'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Feature 1'</span>)</span><br><span class="line">plt.legend([<span class="string">'Class 0'</span>, <span class="string">'Class 1'</span>, <span class="string">'Class 2'</span>], loc=(<span class="number">1.01</span>, <span class="number">0.3</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/5787343-5c7e8ba29af1d512.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>在这里，线条的颜色与各点的颜色是一致的。从图中可以很直观的看到这三个点是如何被分成三类的。</p><p>但是，这三条线交叉的地方，有一个空白的三角区，那这个区域属于哪个类别呢？</p><p><strong>答案是分类方程结果最大的那个类别，即最接近的那条结对应的类别！</strong></p><p>下面将展示整个二维空间是如何被分类的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mglearn.plots.plot_2d_classification(linear_svm, X, alpha=<span class="number">.7</span>)</span><br><span class="line">mglearn.discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], y)</span><br><span class="line">line = np.linspace(<span class="number">-15</span>, <span class="number">15</span>)</span><br><span class="line"><span class="keyword">for</span> coef, intercept, color <span class="keyword">in</span> zip(linear_svm.coef_, linear_svm.intercept_, [<span class="string">'b'</span>, <span class="string">'r'</span>, <span class="string">'g'</span>]):</span><br><span class="line">    plt.plot(line, -(line * coef[<span class="number">0</span>] + intercept) / coef[<span class="number">1</span>], c=color)</span><br><span class="line">plt.xlabel(<span class="string">'Feature 0'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Feature 1'</span>)</span><br><span class="line">plt.legend([<span class="string">'Class 0'</span>, <span class="string">'Class 1'</span>, <span class="string">'Class 2'</span>], loc=(<span class="number">1.01</span>, <span class="number">0.3</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/5787343-a41febbdb4de802f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>通过上图很明白的就看出中间的三角区是如何被分类的了！</p>]]></content>
      
      
      
        <tags>
            
            <tag> sklearn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>量化投资入门指南-视频教程</title>
      <link href="/2019/10/10/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/"/>
      <url>/2019/10/10/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<p><a href="https://study.163.com/course/introduction/1209189848.htm" target="_blank" rel="noopener">课程链接-网易云课堂</a></p><p>适用人群<br>量化交易入门者。</p><p>课程概述<br>没有Python基础，没有量化概念，都不是问题，本课程是专门为编程零基础、量化交易零基础者而打造。利用当下最火热的编程语言Python，对证券标的的价格信息进行分析处理，一步一步完成策略的编写！</p><p><img src="https://edu-image.nosdn.127.net/834a67e236434f79aaeee2c4a9009bd2.jpg?imageView&quality=100&type=webp" alt></p><p><img src="https://edu-image.nosdn.127.net/0202b218bf364517ae5eba6f6be70dea.jpg?imageView&quality=100&type=webp" alt></p><p><img src="https://edu-image.nosdn.127.net/5f767ae3da5944a18c3c84fce3930651.jpg?imageView&quality=100&type=webp" alt></p><p></p>]]></content>
      
      
      
        <tags>
            
            <tag> quant </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Abount me</title>
      <link href="/2019/10/10/AboutMe/"/>
      <url>/2019/10/10/AboutMe/</url>
      
        <content type="html"><![CDATA[<h3 id="博主信息"><a href="#博主信息" class="headerlink" title="博主信息"></a>博主信息</h3><ul><li>名字：石起起</li><li>网名：东南有大树</li><li>籍贯：山西</li><li>现居地：北京</li><li>座右铭：纸上得来终觉浅，绝知此事要躬行</li><li>编程领域<ul><li>主攻[c#, python, .net框架, sklearn]</li><li>涉及领域[numpy, pandas, matplotlib, seaborn, pyecharts, winform, asp.net, sqlserver, mysql, mongodb, html/css, js/jq，tensor flow，scrapy, django]等</li></ul></li><li>Email: <a href="mailto:mrshiqiqi@126.com" target="_blank" rel="noopener">mrshiqiqi@126.com</a>; <a href="mailto:sqqlove123@163.com" target="_blank" rel="noopener">sqqlove123@163.com</a></li><li>Github：<a href="https://github.com/mrshiqiqi" target="_blank" rel="noopener">https://github.com/mrshiqiqi</a>; <a href="https://github.com/helloshiqiqi" target="_blank" rel="noopener">https://github.com/helloshiqiqi</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> life </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
